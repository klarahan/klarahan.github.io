---
  title: "Chapter 5: Framing Welfare"
  subtitle: "Online Appendix"
---
  
```{r include=FALSE}
setwd("~/Dropbox/0 MYCOMP/0_Book manuscript/Lexington/klarahan.github.io")
```

```{r}
library(lubridate)
library(ggplot2)
library(dplyr)
library(tidytext)
```

```{r eval=FALSE, include=FALSE}
# chosun <- readxl::read_xls("data/all_issues.xls", 
#                            sheet = "Chosun_Î≥µÏ?Ä") %>%
#   mutate(Newspaper = "Chosun") %>%
#   mutate(Date = ymd(Date))
# chosun$Body <- chosun$Body %>%
#   gsub("<.*?>", " ", ., perl = TRUE) %>%
#   gsub("[^?Ñ±-?Öé|?Öè-?Ö£|Í∞Ä-?û£]", " ", ., perl = TRUE) %>%
#   gsub("....Í∏∞Ïûê|....?äπ?åå?õê", " ", ., perl = TRUE)%>%
#   gsub("Ï¢ÖÏù¥?ã†Î¨∏Î≥¥Í∏?", " ", ., perl = TRUE) %>%
#   stringr::str_squish()
# 
# hani <- bind_rows(readxl::read_xls("data/all_issues.xls", 
#                                    sheet = "Hani_Î≥µÏ?Ä")) %>%
#   mutate(Newspaper = "Hankyoreh") %>%
#   mutate(Date = ymd(Date)) %>%
#   arrange(Date)
# hani$Body <- hani$Body %>%
#   gsub("<.*?>", " ", ., perl = TRUE) %>%
#   gsub("[^?Ñ±-?Öé|?Öè-?Ö£|Í∞Ä-?û£]", " ", ., perl = TRUE) %>%
#   gsub("....Í∏∞Ïûê|....?äπ?åå?õê", " ", ., perl = TRUE) %>%
#   stringr::str_squish()
# 
# hankook <- bind_rows(readxl::read_xls("data/all_issues.xls", sheet = "Hangook_Î≥µÏ?Ä")) %>%
#   mutate(Newspaper = "Hankook") %>%
#   mutate(Date = ymd(Date)) %>%
#   arrange(Date)
# 
# all_news <- bind_rows(chosun, hani) %>%
#   mutate(Government = case_when(Date >= "1990-01-01" & Date <= "1993-02-24" ~ "1990-1993 Roh TW",
#                                 Date >= "1993-02-25" & Date <= "1998-02-24" ~ "1993-1998 Kim YS",
#                                 Date >= "1998-02-25" & Date <= "2003-02-24" ~ "1998-2003 Kim DJ",
#                                 Date >= "2003-02-25" & Date <= "2008-02-24" ~ "2003-2008 Roh MH",
#                                 Date >= "2008-02-25" & Date <= "2013-02-24" ~ "2008-2013 Lee MB",
#                                 Date >= "2013-02-25" & Date <= "2014-12-31" ~ "2013-2014 Park GH")) %>%
#   mutate(Prezparty = case_when(Date >= "1990-01-01" & Date <= "1993-02-24" ~ "Conservative",
#                                 Date >= "1993-02-25" & Date <= "1998-02-24" ~ "Conservative",
#                                 Date >= "1998-02-25" & Date <= "2003-02-24" ~ "Liberal",
#                                 Date >= "2003-02-25" & Date <= "2008-02-24" ~ "Liberal",
#                                 Date >= "2008-02-25" & Date <= "2013-02-24" ~ "Conservative",
#                                 Date >= "2013-02-25" & Date <= "2014-12-31" ~ "Conservative")) 
# rm(chosun, hani)
# 
# all_news %>%
#     filter(stringr::str_detect(Title, "?ä§Î™®Í∑∏"))
# 
# hankook %>%
#     filter(stringr::str_detect(Title, "?ì∞?†àÍ∏∞Ï≤ò?üº Î≤ÑÎ†§ÏßÄ?äî Í≥µÎèôÏ≤¥Ïùò?ãù"))
```

Load processed data
```{r}
# saveRDS(all_news, "data/welfare_news.RDS")
all_news <- readRDS("data/welfare_news.RDS")
```

Count of all articles
```{r}
all_news %>%
  count(Newspaper, Government) %>%
  ggplot(aes(Government, n, fill = Newspaper)) +
  geom_col(position = "dodge") +
  labs(x = "administrations", y = "count of articles") +
  facet_wrap(~Newspaper, ncol = 2, scales = "free_y") +
  scale_fill_grey() +
  theme_bw() +
  scale_x_discrete(labels = shortgov)

```

Save stopwords
```{r}
# save legit stopwords, and add some
stopwords <- tibble(
  tokens = stopwords::stopwords("ko", source = "stopwords-iso"),
  lexicon = "stopwords-iso") 
stopwords <- stopwords %>% 
  add_row(tokens = c("?ì§?ù¥", "?ïòÍ∏?", "Í∑∏Í≤É", "?ïåÎ¨?", "Í≤ÉÏúºÎ°?", "?ûà?äî", "?úÑ?ï¥",  
                     "??Ä?ïú", "?ì±?ùÑ", "?ñà?ã§", "ÎßêÌñà?ã§", "ÎßåÏõê", "?ïú?ã§", "ÏßÄ?Çú", 
                     "Í≤ÉÏ?Ä"," Í≤ÉÏù¥", "?úÑ?ïú", "Í≤ÉÏù¥?ã§", "?ïò?äî", "?ûà?äî", "?ù¥?ùºÍ≥?",
                     "?ù¥?Ç†", "Î∞ùÌòî?ã§", "?ò§?õÑ", "?Üµ?ï¥", "??Ä?ï¥", "Í∑∏Îäî", "?Çò?äî", 
                     "Í≤ÉÏù¥", "?óÜ?ã§", "Í∞ôÏ?Ä", "?Ç¥Í∞Ä", "?ûà?óà?ã§", "Í≤ΩÏö∞", "?ì±?ù¥",
                     "Í∑∏Îäî", "ÏßÄ?ùå", "?ö∞Î¶¨Îäî", "?ïÑ?ãà?ùº", "?óÜ?äî", "Í≤ÉÏùÑ", "?î®?äî",
                     "?ñµ?õê", "?ãπ?ãú", "ÏßÄ?Çú?ï¥", "?òÑ?û¨", "?ù¥?óê", "Í¥Ä?†®", "?ïòÍ≥?", 
                     "Í∑∏Îü∞", "Í∑∏Ïùò", "Í∞Ä?û•", "ÎßéÏ?Ä", "?ûà?äµ?ãà?ã§", "?ùºÍπåÏ?Ä", "?ò§?äî", 
                     "?ùºÎ∂Ä?Ñ∞", "?ò§?†Ñ", "Í∞Ä?ö¥?ç∞", "?ã§?ãú", "?ùº?äî", "Î™®Îì†", "?ùºÍ≥?", 
                     "Í∑∏Í?Ä", "?Ñ∏Í∏?", "Í≤ÉÎèÑ", "?ù¥Î•?", "?ûà?ã§?äî", "?äπ?ûà", "?ù¥?ñ¥",
                     "?ûê?ã†?ùò", "?ïåÎ¨∏Ïù¥?ã§", "ÎßéÏù¥", "?ïäÍ≥?", "?ÉàÎ°úÏö¥", "?ï©?ãà?ã§",
                     "?êú?ã§", "?ì±?ùò", "?ù¥?†áÍ≤?", "?ïÑ?ãà?ã§", "ÏßÄÍ∏?", "Î≥¥Î©¥", "?ûà?ùÑ", 
                     "?ö∞Î¶¨Í?Ä", "?Ç¨?ûå?ì§?ù¥", "?ù¥?†ú", "?ûà?ñ¥", "?ó∞?ã§", "ÏµúÍ∑º", 
                     "Í≥ÑÌöç?ù¥?ã§", "?ò¨?ï¥", "?ù¥?õÑ"), 
          lexicon = "stopwords-iso") %>% 
  rename(word = tokens)
```

Korean sentiment lexicon
```{r emo, echo=FALSE}
#https://sites.google.com/site/datascienceslab/projects/multilingualsentiment
pos <- readr::read_delim("data/positive_words_ko.txt", delim='\t', col_names=c("term")) %>% 
  rename(word = term) %>% 
  tibble::add_column(sentiment = "positive")
neg <- readr::read_delim("data/negative_words_ko.txt", delim='\t', col_names=c("term")) %>% 
  rename(word = term) %>% 
  tibble::add_column(sentiment = "negative")
senti <- bind_rows(pos, neg) 
rm(pos, neg)
```


```{r}

tidy_news <- all_news %>%
  dplyr::mutate(article =  row_number()) %>%
  tidytext::unnest_tokens(word, Body) %>%
  dplyr::anti_join(stopwords)

tidy_news %>%
  inner_join(senti) %>%
  count(word, sort = TRUE) 
```

Count the emotional words
```{r}
senti_news <- tidy_news %>% 
  inner_join(senti) %>%
  count(Newspaper, Prezparty, sentiment) %>%
  tidyr::spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  group_by(Newspaper, Prezparty) %>% 
  summarise(m = mean(sentiment)) %>% 
  mutate(density = m/sum(m)) 

ggplot(senti_news, aes(Prezparty, density, fill = Prezparty)) +
  geom_col(show.legend = FALSE) +
  labs(x = "governments (1990-2014)", y = "% of net positive sentiment words") +
  facet_wrap(~Newspaper, ncol = 2) +
  scale_fill_grey() +
  theme_bw() +
  scale_y_continuous(labels = scales::percent)
```

Did negativity increase in specific administrations?
```{r}
senti_news_admin <- tidy_news %>%
  inner_join(senti) %>%
  count(Newspaper, Government, sentiment) %>%
  tidyr::spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  group_by(Newspaper, Government) %>% 
  summarise(m = mean(sentiment)) %>% 
  mutate(density = m/sum(m)) 

shortgov <- c("1990-1993\nRoh TW", "1993-1998\nKim YS", "1998-2003\nKim DJ", "2003-2008\nRoh MH", "2008-2013\nLee MB", "2013-2014\nPark GH")
  
ggplot(senti_news_admin, aes(Government, density, fill = Newspaper)) +
  geom_col(position = "dodge") +
  labs(x = "administrations", y = "% of net positive sentiment words") +
  # facet_wrap(~Newspaper, ncol = 1, scales = "free_y") +
  scale_fill_grey() +
  theme_bw() +
  scale_y_continuous(labels = scales::percent) +
  scale_x_discrete(labels = shortgov)

```

Sentiment in articles that contain keyword "National Parliament"
```{r}
all_news %>%
  filter(stringr::str_detect(Body, "Íµ??öå")) %>% 
  dplyr::mutate(article =  row_number()) %>%
  tidytext::unnest_tokens(word, Body) %>% 
  dplyr::anti_join(stopwords) %>% 
  inner_join(senti) %>%
  count(Newspaper, Prezparty, Government, sentiment) %>%
  tidyr::spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  group_by(Newspaper, Prezparty) %>% 
  summarise(m = mean(sentiment)) %>% 
  mutate(density = m/sum(m)) %>% 
  ggplot(aes(Prezparty, density, fill = Newspaper)) +
  geom_col(position = "dodge", show.legend = FALSE) +
  labs(x = "party of President", y = "% of net positive sentiment words") +
  facet_wrap(~Newspaper, ncol = 2) +
  scale_fill_grey() +
  theme_bw() +
  scale_y_continuous(labels = scales::percent) 

```

Sentiments in articles that contain keyword "Taxes"
```{r}
all_news %>%
  filter(stringr::str_detect(Body, "?Ñ∏Í∏?")) %>% 
  filter(Date >= "2010-01-01" & Date <= "2014-12-31") %>% 
  mutate(Year = year(Date)) %>% 
  dplyr::mutate(article =  row_number()) %>%
  tidytext::unnest_tokens(word, Body) %>% 
  dplyr::anti_join(stopwords) %>% 
  inner_join(senti) %>%
  count(Newspaper, Prezparty, Year, sentiment) %>%
  tidyr::spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  group_by(Newspaper, Prezparty, Year) %>% 
  summarise(m = mean(sentiment)) %>% 
  mutate(density = m/sum(m)) %>% 
  ggplot(aes(Year, density, fill = Newspaper)) +
  geom_col(position = "dodge", show.legend = FALSE) +
  labs(x = "Year", y = "% of net positive sentiment words") +
  facet_wrap(~Newspaper, ncol = 2) +
  scale_fill_grey() +
  theme_bw() +
  annotate("text", x = 2012, label = "Presidential Election", 
           y = 0.35, size = 4, angle = 90)  +  
  annotate("pointrange", x = 2012, y = 0, ymin = 0, ymax = 0.23,
           colour = "red", size = 0.5) +
  scale_y_continuous(labels = scales::percent) 

```

Sentiments in articles that contain keyword "Taxes" since 1990
```{r}
all_news %>%
  filter(stringr::str_detect(Body, "?Ñ∏Í∏?")) %>% 
  # filter(Date >= "2010-01-01" & Date <= "2014-12-31") %>% 
  mutate(Year = year(Date)) %>% 
  dplyr::mutate(article =  row_number()) %>%
  tidytext::unnest_tokens(word, Body) %>% 
  dplyr::anti_join(stopwords) %>% 
  inner_join(senti) %>%
  count(Newspaper, Prezparty, Year, sentiment) %>%
  tidyr::spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  group_by(Newspaper, Prezparty, Year) %>% 
  summarise(m = mean(sentiment)) %>% 
  mutate(density = m/sum(m)) %>% 
  ggplot(aes(Year, density, fill = Newspaper)) +
  geom_col(position = "dodge", show.legend = FALSE) +
  labs(x = "Year", y = "% of net positive sentiment words") +
  facet_wrap(~Newspaper, ncol = 2) +
  scale_fill_grey() +
  theme_bw() +
  scale_y_continuous(labels = scales::percent) +
  annotate("text", x = 2012, label = "2012 Election", 
           y = 0.2, size = 3, angle = 90) +
  annotate("text", x = 2007, label = "2007 Election", 
           y = 0.2, size = 3, angle = 90) +  
  annotate("pointrange", x = 2012, y = 0, ymin = 0, ymax = 0.15,
           colour = "red", size = 0.5) +  
  annotate("pointrange", x = 2007, y = 0, ymin = 0, ymax = 0.15,
           colour = "red", size = 0.5)  

```

Sentiments in articles that contain keyword "cost-free"
```{r}
all_news %>%
  filter(stringr::str_detect(Body, "Î¨¥ÏÉÅ")) %>% 
  dplyr::mutate(article =  row_number()) %>%
  tidytext::unnest_tokens(word, Body) %>% 
  dplyr::anti_join(stopwords) %>% 
  inner_join(senti) %>%
  count(Newspaper, Prezparty, Government, sentiment) %>%
  tidyr::spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  group_by(Newspaper, Government) %>% 
  summarise(m = mean(sentiment)) %>% 
  mutate(density = m/sum(m)) %>% 
  ggplot(aes(Government, density, fill = Newspaper)) +
  geom_col(position = "dodge") +
  labs(x = "administrations", y = "% of net positive sentiment words") +
  # facet_wrap(~Newspaper, ncol = 2) +
  scale_fill_grey() +
  theme_bw() +
  scale_y_continuous(labels = scales::percent) +
  scale_x_discrete(labels = shortgov)

```

Sentiments in articles that contain keyword "National Assembly" by administrations
```{r}
all_news %>%
  filter(stringr::str_detect(Body, "Íµ??öå")) %>% 
  dplyr::mutate(article =  row_number()) %>%
  tidytext::unnest_tokens(word, Body) %>% 
  dplyr::anti_join(stopwords) %>% 
  inner_join(senti) %>%
  count(Newspaper, Prezparty, Government, sentiment) %>%
  tidyr::spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>% 
  group_by(Newspaper, Government) %>% 
  summarise(m = mean(sentiment)) %>% 
  mutate(density = m/sum(m)) %>% 
  ggplot(aes(Government, density, fill = Newspaper)) +
  geom_col(position = "dodge", show.legend = FALSE) +
  labs(x = "administrations", y = "% of net positive sentiment words") +
  facet_wrap(~Newspaper, ncol = 2) +
  scale_fill_grey() +
  theme_bw() +
  scale_y_continuous(labels = scales::percent) +
  scale_x_discrete(labels = shortgov) +
  theme(axis.text.x = element_text(angle = 45))

```
